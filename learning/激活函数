swish激活函数：
Swish函数先对来说是比较新的一些激活函数，算是由之前的激活函数复合而成出来的。也是由Google提出的，毕竟资力雄厚，承担的起搜索的任务。而且这个算法感觉曝光率还算比较高，就在这里整理一下，同时后面的文章也会再次提到这个函数。
  对前面的激活函数有了一定的基础之后，理解Swish激活就容易很多了，Swish函数的表达式是f(x)=x⋅σ(x)f(x)=x⋅σ(x)，σ(x)σ(x)就是sigmoid函数。因为sigmoid函数的饱和性容易导致梯度消失，借鉴ReLU的效果，当xx非常大的时候，这个时候有f(x)f(x)趋近于xx，但是当x→−∞，则f(x)→0x→−∞，则f(x)→0，函数的大致走势和ReLU比较相似，但是又比ReLU复杂。

geru激活函数：
gelu（gaussian error linear units）就是我们常说的高斯误差线性单元，它是一种高性能的神经网络激活函数，因为gelu的非线性变化是一种符合预期的随机正则变换方式，公式如下：xP(X≤x)=xΦ(x)(2.1)xP(X≤x)=xΦ(x)(2.1)其中Φ(x)Φ(x)指的是xx的高斯正态分布的累积分布，完整形式如下：xP(X≤x)=x∫x−∞e−(X−μ)22σ22π√σdX(2.2)xP(X≤x)=x∫−∞x​2π
​σe−2σ2(X−μ)2​​dX(2.2)计算结果约为：0.5x(1+tanh[2π−−√(x+0.044715x3)])(2.3)0.5x(1+tanh[π2​(x+0.044715x3)])(2.3)或者可以表示为：xσ(1.702x)(2.4)xσ(1.702x)(2.4)由此可知，概率P(X≤x)P(X≤x)（xx可看成当前神经元的激活值输入）,即XX的高斯正态分布ϕ(X)ϕ(X)的累积分布Φ(x)Φ(x)是随着xx的变化而变化的，当xx增大，Φ(x)Φ(x)增大，当x减小，Φ(x)Φ(x)减小，即当xx越小，在当前激活函数激活的情况下，越有可能激活结果为0，即此时神经元被dropout，而当xx越大越有可能被保留

